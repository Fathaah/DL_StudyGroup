{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear_Regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_nnKOx_l7dMm"},"source":["<img src = \"https://i.ibb.co/Qr1V3nX/photo.jpg\" width =100>\n","\n",">>>>>>>>><h1>Tecafe AI Study Group</h1>\n","<br>\n","<br>\n","\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nCSL3VTSLzi9","colab_type":"text"},"source":["Completion of this notebook is mandatory to be eligible for certification at the end of the course."]},{"cell_type":"markdown","metadata":{"id":"oEVi0FTyKpqd","colab_type":"text"},"source":["#LINEAR REGRESSION\n","\n","In this notebook we will try to implement our own linear regression model. \n"]},{"cell_type":"markdown","metadata":{"id":"_lGpU2vFLErU","colab_type":"text"},"source":["While creating a ML/DL model the following elements play a crucial role in making your model work.They are:\n","\n","\n","*   DATA\n","*   MODEL\n","*   LOSS FUNCTION\n","*   OPTIMIZER\n","*   TRAINING\n","*    EVALUATION OF THE MODEL (We are not gonna cover this here)\n","\n","For the time being we could come up with only these five. Feel free to add other things in your list and share it with us if you come across something important."]},{"cell_type":"markdown","metadata":{"id":"SaNO0uoyEGAw","colab_type":"text"},"source":["##IMPORTING EVERYTHING WE NEED"]},{"cell_type":"code","metadata":{"id":"8DeaeJ5bEFt5","colab_type":"code","colab":{}},"source":["import numpy as np #Here we are numpy and we can access its functions using np\n","import matplotlib.pyplot as plt #Plotting library"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lmSChJrMHwv","colab_type":"text"},"source":["# 1.DATA"]},{"cell_type":"markdown","metadata":{"id":"Vp2aLlfNEeqZ","colab_type":"text"},"source":["Data is first foremost step in any ML task, at times you get it from a third party and can work with that but other times you will have to do the hard work collecting relevent and clean data.\n","\n","Our objective here is to create and learn about a simple linear regression model. Hence we will generate few data points and it is plotted below."]},{"cell_type":"code","metadata":{"id":"tjE_PuN8FLgj","colab_type":"code","colab":{}},"source":["x = 10 * np.random.rand(50)  #np. random.randn will generate 50 numbers from the normal distribution, then we are scaling it with 10 so that our range becomes 0 to 10.\n","                              #You can read more about how this operation works when we talk about broadcasting.\n","y = 2 * x + 5 + np.random.randn(50)*2   #We are choosing 2 as a slope and 5 as the intercept, then to make it more realistic we are adding random normal numbers again. \n","plt.scatter(x, y)  #Pre defined function to scatter plot points in the array x, y\n","plt.show()   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K0Bo0g6lFgZl","colab_type":"text"},"source":["Perfect now we have our data lets proceed to our next step."]},{"cell_type":"markdown","metadata":{"id":"5qe0zJVpF6Cc","colab_type":"text"},"source":["#2.MODEL"]},{"cell_type":"markdown","metadata":{"id":"cNJ83DJOF9yp","colab_type":"text"},"source":["There are many option when it comes to choosing a model, you need to understand your data and decide if your data will perform well on a particular model.\n","\n","Since we are learning about linear reggression and our data kind of ressembles a line we will go with a simple linear regression model.\n","\n","Here we want to get a value $y$ for any $x$.\n","\n",">$y = w \\cdot x + b$"]},{"cell_type":"markdown","metadata":{"id":"SG8kLHGB5h_J","colab_type":"text"},"source":["**TASK #1**\n","Complete the given code snippet."]},{"cell_type":"code","metadata":{"id":"EbOfeOP4F9Qs","colab_type":"code","colab":{}},"source":["# This code is straightforward I think\n","def model(x,w,b):\n","  prediction = #YOUR CODE GOES HERE\n","  return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KdcM6FGdHB-o","colab_type":"text"},"source":["# 3.COST FUNCTION"]},{"cell_type":"markdown","metadata":{"id":"-GLMKpZ5lV8C","colab_type":"text"},"source":["![alt text](https://i.ytimg.com/vi/zPG4NjIkCjc/maxresdefault.jpg)"]},{"cell_type":"markdown","metadata":{"id":"GURIO1UnmAv_","colab_type":"text"},"source":["So our aim is to find parameters (W and B) of the line passing through those green points in the image given above."]},{"cell_type":"markdown","metadata":{"id":"zSTyzFAtTDcX","colab_type":"text"},"source":["![alt text](https://cdn-images-1.medium.com/max/1000/1*wQCSNJ486WxL4mZ3FOYtgw.png)"]},{"cell_type":"markdown","metadata":{"id":"qt0y5fE0mU2W","colab_type":"text"},"source":["The first equation given above is our objective. The second equation is our \"COST\" function.\n","\n","n * Cost function = Our total \"LOSS\" \n","\n","In other words, our objective is to minimise Cost function."]},{"cell_type":"markdown","metadata":{"id":"S-8vdkQx9WIV","colab_type":"text"},"source":["**TASK #2**\n","Complete the implementation of the loss function below."]},{"cell_type":"code","metadata":{"id":"-E0P6PVnHHYg","colab_type":"code","colab":{}},"source":["# Pythonic way for finding cost\n","\n","def compute_cost(x,y,w,b):\n","  '''\n","  \n","  compute_cost function has the following arguments.\n","  x : Your all x values or input features (type: list).\n","  y : Labels or your expected outputs (type: list).\n","  w : It is the slope of the line (type: Float).\n","  b : Bias or the intercept of the line (type : float).\n","  \n","  You may use the model() function in order to solve this task.\n","  \n","  '''\n","  n = len(y)\n","  total_cost = 0  #Initializing the total_cost which will be the value of summation.\n","  for i in range(n):  #To permform summation.\n","    #Your code goes here\n","  cost = total_cost / n\n","  return 2\n","\n","#Veriy if your function is working properly\n","if compute_cost([2],[1],2,1) != 16:\n","  print(\"Your Fuction has some issues, try again\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQpRcToU6q21","colab_type":"text"},"source":["You can also do this in the following way."]},{"cell_type":"markdown","metadata":{"id":"aqwNSGqQ-Qrl","colab_type":"text"},"source":["\n","\n","```\n","cost = (np.sum(np.square(model(x,w,b)-y)))/len(y)\n","```\n","The above statement is an example of broadcasting in python. It is an essential part of programming with big data.\n","You can refer more about it [here](https://machinelearningmastery.com/broadcasting-with-numpy-arrays/) or [here](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html).\n"]},{"cell_type":"code","metadata":{"id":"ftgFXV-2pZb3","colab_type":"code","colab":{}},"source":["# Our favourite numpy style\n","\n","def compute_cost_numpy(x,y,w,b):\n","  cost = (np.sum(np.square(model(x,w,b)-y)))/len(y)   \n","  return cost"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I8WxETvdHwfY","colab_type":"text"},"source":["# 4.OPTIMIZER"]},{"cell_type":"markdown","metadata":{"id":"ePlw5ranq_5k","colab_type":"text"},"source":["Below are some theory. We have tried to explain in more detail and have tried our best to eliminate your confusion."]},{"cell_type":"markdown","metadata":{"id":"ueuxM8lNoOma","colab_type":"text"},"source":[">$\\hat{y_i} = w * x_i + b$\n","\n","Cost function\n","\n",">$J = \\frac{1}{n}\\cdot\\sum_{i=1}^n (\\hat{y_i} - y_i)^2$\n","\n","And our objective is to minimise it.\n","Differentiating partially with respect to $w$, we get -\n","\n",">$dw = \\frac{2}{n} \\cdot \\sum_{i=1}^n (\\hat{y_i} - y_i) \\cdot x_i$\n","\n","Similarly, we can find the partial derivative with respect to the bias $b$.\n","\n",">$db = \\frac{2}{n} \\cdot \\sum_{i=1}^n (\\hat{y_i} - y_i)$\n","\n","<br> \n","\n","The partial derivative points in the direction of steepest ascent from that point.<br> Therefore, negative of a derivtive should point the direction of steepest descent.<br> Using this concept, we can find new w as\n","\n",">$w = $ W $ - \\ \\alpha \\cdot (dw)$\n","\n","Where  $\\alpha$, is called learning rate which decides the step size. \n","\n","Similarly, we can find  bias $b$.\n","\n",">$b = $ B $ - \\ \\alpha \\cdot (db)$\n","\n","We find $dw$ and $db$ each and every point during the descent and update w and b until both the derivatives become close to zero( which is almost equivalent to saying loss becomes close to zero). Below you can find a plot of a simple cost function, but usually the cost functions are more complex with multiple maximas and minimas.\n","\n","\n","\n","Note $db$ and $dw$ are equivalent to $\\frac{dJ}{db}$ and $\\frac{dJ}{dw}$ respectively"]},{"cell_type":"markdown","metadata":{"id":"TvJ-FGtg6AGL","colab_type":"text"},"source":["![alt text](https://i.imgur.com/xnPvEok.gif)\n","![](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization_files/ball.png)"]},{"cell_type":"markdown","metadata":{"id":"auZbBW2zCbf4","colab_type":"text"},"source":["**TASK #3**\n","\n","Perform the above calculations in the below function."]},{"cell_type":"code","metadata":{"id":"KsxTIYd2S8DF","colab_type":"code","colab":{}},"source":["def grad_update(x,y,w,b,learning_rate = 0.01):\n","  #Your code goes here \n","  return w,b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8ZZ99_nDzSb","colab_type":"text"},"source":["#5.TRAINING\n","\n","We follow these steps:\n","\n","\n","1.   Firstly assign a random value for $w$ and $b$ to begin our prediction.\n","\n","2.   Using the $w$ and $b$ we make a prediction for $\\hat{y}$.\n","\n","3.   Calculate the loss $J$, by checking how different our predicted $\\hat{y}$  is from $y$ using \n",">$J = \\frac{1}{n}\\cdot\\sum_{i=1}^n (\\hat{y_i} - y_i)^2$\n","\n","4.   Then we find the derivative with respect to $w$ and $b$.\n","\n","5.   Next we update the $w$ and $b$, then repeat steps 2 to 5 .\n","\n"]},{"cell_type":"code","metadata":{"id":"L75PJs1si_Z4","colab_type":"code","colab":{}},"source":["W,B = 0,0   # Initialise both W and B to be 0\n","\n","p = model(x,W,B) # Get the predictions \n","\n","plt.plot(x,p,'r-')  # The output predicted by our model is red line\n","plt.scatter(x,y)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nizQY4UjSViN"},"source":["As you can see, initially we will start off with that red line.... (W=0 and B=0)"]},{"cell_type":"code","metadata":{"id":"uqRTMMsyMgWd","colab_type":"code","colab":{}},"source":["w = 0\n","b = 0\n","new_w = 0\n","new_b = 0\n","n = len(y)\n","\n","for i in range(10000):    # Assume that your model will reach minima by 10000 iterations\n","  if i%10 == 0:\n","    print(compute_cost(x,y,w,b))  \n","  w,b = grad_update(x,y,w,b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"URb-ewcMWK_a","colab_type":"code","colab":{}},"source":["w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2yQkQUUZF-z","colab_type":"code","colab":{}},"source":["b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tq680sJPSchE","colab":{}},"source":["plt.plot(x, y, 'ro', label='Original data')\n","plt.plot(x, w * x + b, label='Fitted line')\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMYK6PkwROiF","colab_type":"text"},"source":["###Using numpy and another approach"]},{"cell_type":"markdown","metadata":{"id":"sYIeT9gd98Os","colab_type":"text"},"source":["This is optional you may skip this."]},{"cell_type":"code","metadata":{"id":"ZUM8QG8CSlaQ","colab_type":"code","colab":{}},"source":["threshold = 0.84\n","lr = 0.0003   # Learning Rate\n","\n","loss = compute_cost_numpy(x,y,W,B)\n","\n","loss_history = [loss] \n","\n","print(\"Training started\")\n","\n","c = 0\n","\n","while (loss>threshold):#Wait till the loss goes below our desired threshold\n","  \n","  W,B = gradient_descent(x,y,W,B,lr)\n","  loss = compute_cost_numpy(x,y,W,B)\n","  \n","  if c%100==0:\n","    loss_history.append(loss)\n","    print(\"loss=\",loss)\n","    c=1\n","  else:\n","    c+=1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_NgViagYnAK","colab_type":"code","colab":{}},"source":["# Let's plot the loss function\n","\n","plt.plot(loss_history,np.arange(len(loss_history)))\n","plt.ylabel(\"Loss\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9V3v1zt0ZJIK","colab_type":"code","colab":{}},"source":["# Let's see what are the values of W and B we got\n","\n","W"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"079uR5bGZP4l","colab_type":"code","colab":{}},"source":["B"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EycRIiNnD_wv","colab_type":"text"},"source":["#USING SCIKIT LEARN\n","\n","Dont worry each time you want to implement a linear regression model you dont need to code this much. You can use readily available libraries to do so.\n"]},{"cell_type":"code","metadata":{"id":"x4E32qFMERhR","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LinearRegression\n","x = [[1],[2],[5],[6]]\n","y = np.dot(x,4)+4\n","reg  = LinearRegression()\n","model = reg.fit(x,y)\n","print(model.coef_,model.intercept_ )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AMHD2jeNEg_h","colab_type":"text"},"source":["You can learn more about SciKit learns linear regression model   [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."]},{"cell_type":"markdown","metadata":{"id":"AuVpJ2jAZZ1J","colab_type":"text"},"source":["If you go back to the top and see the equation, the equation we gave was y = 2*x + 5 .\n","To this we added some random values from normal distribution with mean=0 ,\n","Makes sense right?"]},{"cell_type":"markdown","metadata":{"id":"FQ31iyBccCV1","colab_type":"text"},"source":["\n","This is sometimes called a simple perceptron model with a single feature.\n","\n","![alt text](https://www.lucidarme.me/wp-content/uploads/2017/12/perceptron-2.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wZGanhDCJcZR","colab_type":"text"},"source":["# You have coded your first ML model, Congrats!!\n","\n","You can send your own copy of this to\n","\n","##nitc.tecafe@gmail.com"]}]}